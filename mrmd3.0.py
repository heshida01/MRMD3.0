# !/usr/bin/env python3
# -*- coding=utf-8 -*-


def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn

import pandas as pd
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import LinearSVC
import argparse
import sklearn.metrics
import time
import logging
import os,math
from scipy.io import arff
from sklearn.datasets import load_svmlight_file
from sklearn.datasets import dump_svmlight_file
from format import pandas2arff
from sklearn.manifold import TSNE
from math import ceil
from sklearn.preprocessing import LabelBinarizer,MinMaxScaler
from feature_rank import feature_rank
from sklearn.cluster import  *
from utils.eigen_decomposition import ed_run
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier

np.random.seed(1)

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-s", "--start", dest='s', type=int, help="start index", default=1)
    parser.add_argument("-i", "--inputfile", dest='i', type=str, help="input file", required=True)
    parser.add_argument("-e", "--end", dest='e', type=int, help="end index", default=-1)
    parser.add_argument("-l", "--length", dest='l', type=int, help="step length", default=1)
    parser.add_argument("-n", "--n_dim", dest='n', type=int, help="mrmd3.0 features top n", default=-1)
    parser.add_argument("-t", "--type_metric", dest='t', type=str, help="evaluation metric(f1, acc, recall,precision,  auc)", default="f1")
    # parser.add_argument("-m", "--metrics_file", dest='m', type=str, help="output the metrics file", default=None)
    parser.add_argument("-c", "--classifier", dest='c', type=str, help="classifier(RandomForest,SVM,Bayes)",
                        default="RandomForest", choices=["RandomForest", "SVM", "Bayes"])
    parser.add_argument("-o", "--outfile", dest='o', type=str, help="output the dimensionality reduction file")
    parser.add_argument("-p", "--picture", dest='p', type=str, default='true',
                        help="The scatter plots before and after dimension reduction are generated by tsne")
    parser.add_argument("-f","--topn", dest='f', type=int, default=15,
                        help="select top n features to chart")
    parser.add_argument("-r", "--rank_method", dest='r', type=str, help="the rank method for features",
                        choices=["PageRank",
                                 "Hits_a",
                                 "Hits_h",
                                 "LeaderRank",
                                 "TrustRank"],
                        default="PageRank")
    args = parser.parse_args()

    return args


class Dim_Rd(object):
    def __init__(self, file_csv, logger):
        self.file_csv = file_csv
        self.logger = logger


    def read_data(self):  # default csv

        def read_csv():
            self.df = pd.read_csv(self.file_csv, engine='python').dropna(axis=1)
            datas = np.array(self.df)
            self.datas = datas
            self.X = datas[:, 1:]
            self.y = datas[:, 0]

        file_type = self.file_csv.split('.')[-1]
        if file_type == 'csv':
            read_csv()

    def range_steplen(self, start=1, end=1, length=1):
        self.start = start
        self.end = end
        self.length = length

    def classifier(self, X, y):
        if args.c == "SVM":
            clf = LinearSVC(random_state=1, tol=1e-5)
        elif args.c == "Bayes":
            clf = GaussianNB()
        else:
            clf = RandomForestClassifier(random_state=1, n_estimators=100, n_jobs=-1)
        # cv_results=cross_validate(clf,X,y,return_train_score=False,cv=10,n_jobs=-1)

        ypred = sklearn.model_selection.cross_val_predict(clf, X, y, n_jobs=-1, cv=5)
        f1 = sklearn.metrics.f1_score(y, ypred, average='weighted')
        precision = sklearn.metrics.precision_score(self.y, ypred, average='weighted')
        recall = sklearn.metrics.recall_score(self.y, ypred, average='weighted')
        acc = sklearn.metrics.accuracy_score(self.y, ypred)
        lb = LabelBinarizer()
        lb.fit(self.y)

        y = lb.transform(self.y)
        ypred = lb.transform(ypred)
        auc = sklearn.metrics.roc_auc_score(y, ypred)

        return acc, f1, precision, recall, auc, ypred

    def Result(self, seqmax, clf, features, csvfile):
        ypred = sklearn.model_selection.cross_val_predict(clf, self.X[:, seqmax], self.y, n_jobs=-1, cv=5)

        cm = pd.crosstab(pd.Series(self.y, name='Actual'), pd.Series(ypred, name='Predicted'))
        confusion_matrix = str('***confusion matrix***' + os.linesep + str(cm))
        logger.info(confusion_matrix)
        f1 = sklearn.metrics.f1_score(self.y, ypred, average='weighted')
        # f.write('f1 ={}\n '.format(f1))
        logger.info(('f1 ={:0.4f} '.format(f1)))
        acc = sklearn.metrics.accuracy_score(self.y, ypred)
        logger.info('accuarcy = {:0.4f} '.format(acc))
        # f.write('accuarcy = {:} \n'.format(acc))
        precision = sklearn.metrics.precision_score(self.y, ypred, average='weighted')
        logger.info('precision ={:0.4f} '.format(precision))
        # f.write('precision ={} \n'.format(precision))
        recall = sklearn.metrics.recall_score(self.y, ypred, average='weighted')
        logger.info(('recall ={:0.4f}'.format(recall)))
        # f.write('recall ={}\n '.format(recall))
        lb = LabelBinarizer()
        y = lb.fit_transform(self.y)
        ypred = lb.transform(ypred)
        auc = sklearn.metrics.roc_auc_score(y, ypred)
        # f.write('roc area = {}\n'.format(roc))
        logger.info('auc = {:0.4f}'.format(auc))

        columns_index = [0]
        columns_index.extend([i + 1 for i in seqmax])
        data = np.concatenate((self.y.reshape(len(self.y), 1), self.X[:, seqmax]), axis=1)
        features_list = (self.df.columns.values)

        if args.n == -1:
            pass
        else:
            columns_index = columns_index[0:args.n + 1]
            data = data[:, 0:args.n + 1]
        df = pd.DataFrame(data, columns=features_list[columns_index])
        df.iloc[0, :].astype(int)
        df.to_csv(csvfile, index=None)

    def Dim_reduction(self, features, features_sorted, outfile, csvfile):
        logger.info("Start dimension reduction ...")
        features_number = []
        for value in features_sorted:
            features_number.append(features[value[0]] - 1)
        stepSum = 0
        max = 0
        seqmax = []
        predmax = []
        scorecsv = outfile

        with open(scorecsv, 'w') as f:
            f.write('length,accuracy,f1,precision,recall,auc\n')
            for i in range(int(ceil((self.end - self.start) / self.length)) + 1):
                if (stepSum + self.start) < self.end:
                    n = stepSum + self.start
                else:
                    n = self.end

                stepSum += self.length

                ix = features_number[self.start - 1:n]
                acc, f1, precision, recall, auc, ypred = self.classifier(self.X[:, ix], self.y)

                if args.t == "f1":
                    benchmark = f1
                elif args.t == "acc":
                    benchmark = acc
                elif args.t == "precision":
                    benchmark = precision
                elif args.t == "recall":
                    benchmark = recall
                elif args.t == "auc":
                    benchmark = auc

                if benchmark > max:
                    max = benchmark
                    seqmax = ix

                logger.info(
                    'length={} f1={:0.4f} accuarcy={:0.4f} precision={:0.4f} recall={:0.4f} auc={:0.4f} '.format(
                        len(ix), f1, acc, precision, recall, auc))
                f.write('{},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f}\n'.format(len(ix), acc, f1, precision, recall, auc))

        logger.info('----------')
        logger.info('the max {} = {:0.4f}'.format(args.t, max))

        index_add1 = [x + 1 for x in seqmax]
        logger.info('{},length = {}'.format(self.df.columns.values[index_add1], len(seqmax)))
        logger.info('-----------')
        if args.c == "SVM":
            clf = LinearSVC(random_state=1, tol=1e-5)
        elif args.c == "Bayes":
            clf = GaussianNB()
        else:
            clf = RandomForestClassifier(random_state=1, n_estimators=100, n_jobs=-1)
        self.Result(seqmax, clf, features, csvfile)
        logger.info('-----------')

    def Dim_reduction_cluster(self, features, features_sorted, outfile, csvfile):
        """
        features: OrderedDict([('A', 1), ('C', 2), ('D', 3), ('E', 4), ('F', 5), ('G', 6), ('H', 7), ('I', 8), ('K', 9), ('L', 10), ('M', 11), ('N', 12), ('P', 13), ('Q', 14), ('R', 15), ('S', 16), ('T', 17), ('V', 18), ('W', 19), ('Y', 20)])
        features_soretd [('D', 0.0637252657030004), ('Q', 0.06258806931970864), ('S', 0.06031562918662554), ('G', 0.058283810590057876), ('E', 0.05792550137223721), ('R', 0.057592089711556715), ('K', 0.05407480663088009), ('H', 0.052265259431547084), ('V', 0.051791040145125096), ('N', 0.05084200161558971), ('C', 0.05053386395330025), ('A', 0.04940861832440385), ('L', 0.04664026303618639), ('Y', 0.04599471159855803), ('P', 0.045468837115498324), ('I', 0.04535490829739398), ('F', 0.040946128728879136), ('W', 0.03955590400504348), ('T', 0.03865878722016879), ('M', 0.02803450401423936)]
        """
        logger.info("Start dimension reduction ...")
        features_sorted = [x[0] for x in features_sorted]
        new_order_features_num = []
        for x in features_sorted:
            new_order_features_num.append(features[x]-1)
        X = self.df.iloc[:,1:]
        y = self.df.iloc[:,0]


        minmaxscaler = MinMaxScaler()
        X= minmaxscaler.fit_transform(np.array(X))
        ks = ed_run(np.array(X).T,30)
        print("ks",ks)

        best_metric = 0
        best_features = []
        for k in ks:
            t = []
            t2 = []
            sc_cluster = SpectralClustering(n_clusters=k)
            sc_cluster.fit(np.array(X).T)
            # print(ks,np.array(X).T.shape)
            # print(k,sc_cluster.labels_)
            new_sc_labels = sc_cluster.labels_[np.array(new_order_features_num)]
            for x in zip(new_sc_labels,features_sorted):
                if x[0] not in t:
                    t.append(x[0])
                    t2.append(x[1])
            # print(t) #             [2, 0, 1]
            # print(t2)# ['attributeFeature79', 'attributeFeature152', 'attributeFeature99']

            clf = RandomForestClassifier(random_state=1, n_estimators=100, n_jobs=-1)
            ypred = sklearn.model_selection.cross_val_predict(clf, self.df.iloc[:,1:].loc[:,t2], y, n_jobs=-1, cv=5)
            metric = sklearn.metrics.f1_score(y, ypred)

            if best_metric < metric:
                best_metric = metric
                best_features = t2
            print(t2,metric)

        print("###",len(best_features),best_metric)
        ###
        clf = RandomForestClassifier(random_state=1, n_estimators=100, n_jobs=-1)
        ypred = sklearn.model_selection.cross_val_predict(clf, self.df.iloc[:, 1:], y, n_jobs=-1, cv=5)
        metric = sklearn.metrics.f1_score(y, ypred)
        print("metric", f"{len(self.df.columns)-1 } ",metric)
        ###
    def run(self, inputfile):

        args = parse_args()
        file = inputfile
        # if args.m == None:
        #     args.m = ''.join(os.path.basename(args.i).split('.')[:-1]) + '.metrics.csv'

        metrics_file = ''.join(os.path.basename(args.i).split('.')[:-1]) + '.metrics.csv'
        csvfile = args.o

        features, features_sorted = feature_rank(file, self.logger,args.r,0,-1)
        #### OrderedDict([('A', 1), ('C', 2), ('D', 3), ('E', 4), ('F', 5), ('G', 6), ('H', 7), ('I', 8), ('K', 9), ('L', 10), ('M', 11), ('N', 12), ('P', 13), ('Q', 14), ('R', 15), ('S', 16), ('T', 17), ('V', 18), ('W', 19), ('Y', 20)])
        ####  [('Q', 0.4135146288735066), ('Y', 0.39893698213692674), ('S', 0.3877355423066138), ('A', 0.37869036611873325), ('G', 0.3585291694093776), ('R', 0.3564785894669215), ('C', 0.33684050001203647), ('N', 0.3128496649686209), ('E', 0.31253485677608217), ('D', 0.3084676327695345), ('I', 0.3069034315358833), ('P', 0.2686084169563175), ('H', 0.2633607086324316), ('K', 0.21717671201197009), ('T', 0.21567079046297372), ('F', 0.20491791357491876), ('L', 0.2039444517457154), ('V', 0.18273791283791718), ('M', 0.15901523300208054), ('W', 0.1585000287076487)]
        self.read_data()
        if int(args.e) == -1:
            args.e = len(pd.read_csv(file, engine='python').columns) - 1
        self.range_steplen(args.s, args.e, args.l)
        metrics_file = os.getcwd() + os.sep + 'Results' + os.sep + metrics_file
        csvfile = os.getcwd() + os.sep + 'Results' + os.sep + csvfile


        self.Dim_reduction(features, features_sorted, metrics_file, csvfile)
        return features_sorted


def arff2csv(file):

    data = arff.loadarff(file)
    df = pd.DataFrame(data[0])
    df['class'] = df['class'].map(lambda x: x.decode())

    # eg: 0  1    2     3     4  mean =>>  mean   0     1     2    3    4 in dataframe
    cols = df.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    df = df[cols]

    file_csv = file + '.csv'
    df.to_csv(file_csv, index=None)
    return file_csv

def arff2csv_notfile(file):
    with open(file,"r+") as f:
        conent = []
        for line in  f.readlines():
            if line:
                if "{" in line:
                    line = line.replace("{"," {")
                    conent.append(line)
                else:
                    conent.append(line)
    with open(file,"w+") as f:
        for line in  conent:
            f.write(line)

    data = arff.loadarff(file)
    df = pd.DataFrame(data[0])
    df['class'] = df['class'].map(lambda x: x.decode())

    # eg: 0  1    2     3     4  mean =>>  mean   0     1     2    3    4 in dataframe
    cols = df.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    df = df[cols]

    return df

def libsvm2csv_notfile(file):
    data = load_svmlight_file(file)
    df = pd.DataFrame(data[0].todense())
    df['class'] = pd.Series(np.array(data[1])).astype(int)
    # eg: 0  1    2     3     4  mean =>>  mean   0     1     2    3    4 in dataframe
    cols = df.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    df = df[cols]

    return df

def libsvm2csv(file):
    data = load_svmlight_file(file)
    df = pd.DataFrame(data[0].todense())
    df['class'] = pd.Series(np.array(data[1])).astype(int)

    # eg: 0  1    2     3     4  mean =>>  mean   0     1     2    3    4 in dataframe
    cols = df.columns.tolist()
    cols = cols[-1:] + cols[:-1]
    df = df[cols]

    file_csv = file + '.csv'
    df.to_csv(file_csv, index=None)

    return file_csv


def tsne_scatter(file):
    df = pd.read_csv(file, engine='python')
    tsne = TSNE()

    label_name = df.columns.values[0]
    fea_data = df.drop(columns=[label_name])  # 取出所有特征向量用于降维
    redu_fea = tsne.fit_transform(fea_data)  # 将数据降到2维进行后期的可视化处理
    labels = df.iloc[:, 0]
    redu_data = np.vstack((redu_fea.T, labels.T)).T  # 将特征向量和正反例标签整合
    tsne_df = pd.DataFrame(
        data=redu_data, columns=['Dimension1', 'Dimension2', "label"])

    scaler = MinMaxScaler()
    tsne_df[['Dimension1', 'Dimension2']] = scaler.fit_transform(tsne_df[['Dimension1', 'Dimension2']])
    p1 = tsne_df[(tsne_df.iloc[:, 2] == 1)]
    p2 = tsne_df[(tsne_df.iloc[:, 2] == 0)]
    x1 = p1.values[:, 0]
    y1 = p1.values[:, 1]
    x2 = p2.values[:, 0]
    y2 = p2.values[:, 1]

    # 绘制散点图
    plt.plot(x1, y1, 'o', color="#3dbde2", label='positive', markersize='1')
    plt.plot(x2, y2, 'o', color="#b41f87", label='negative', markersize='1')
    plt.xlabel('Dimension1', fontsize=9)
    plt.ylabel('Dimension2', fontsize=9)

    plt.legend(loc="upper right", fontsize="x-small")


if __name__ == '__main__':

    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    log_path = os.getcwd() + os.sep + 'Logs' + os.sep
    rq = time.strftime('%Y%m%d%H%M', time.localtime(time.time()))
    log_name = log_path + rq + '.log'
    logfile = log_name
    fh = logging.FileHandler(logfile, mode='w')
    fh.setLevel(logging.INFO)

    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)

    # logging.basicConfig(level=logging.INFO,
    #                     format='[%(asctime)s]: %(message)s')  # logging.basicConfig函数对日志的输出格式及方式做相关配置
    formatter = logging.Formatter('[%(asctime)s]: %(message)s')
    # 文件
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    # 控制台
    ch.setFormatter(formatter)
    logger.addHandler(ch)
    logger.info("---mrmd 3.0 start----")

    args = parse_args()
    file = args.i
    file_type = file.split('.')[-1]
    if file_type == 'csv':
        pass
    elif file_type == 'arff':
        file = arff2csv(file)
    elif file_type == 'libsvm':
        file = libsvm2csv(file)
    else:
        assert "format error"
    # format : arff or libsvm to csv
    if args.p != 'false':
        plt.figure(figsize=(2 * 4.7, 1 * 4.7))
        plt.subplot(1, 2, 1)
        tsne_scatter(file)

    if int(args.e) == -1:
        args.e = len(pd.read_csv(file, engine='python').columns) - 1

    metricfile = ''.join(os.path.basename(args.i).split('.')[:-1]) + '.metrics.csv'

    d = Dim_Rd(file, logger)
    features_sorted = d.run(inputfile=file)
    metrics_file = os.getcwd() + os.sep + 'Results' + os.sep + metricfile
    csvfile = os.getcwd() + os.sep + 'Results' + os.sep + args.o
    logger.info("The output by the terminal's log has been saved in the {}.".format(logfile))
    logger.info('metrics have been saved in the {}.'.format(metrics_file))
    if args.p != 'false':
        plt.subplot(1, 2, 2)
        tsne_scatter(csvfile)
       # plt.title("Comparison before and after dimensionality reduction (TSNE)")
        pngpath = os.path.abspath('./Results') + os.sep + os.path.splitext(os.path.basename(args.i))[0] + '.tsne.png'
        plt.savefig(pngpath)
        logger.info('Scatter charts visualized by t-SNE dataset has been saved in the {}.'.format(pngpath))

    # 处理输出文件的类型
    outputfile_file_type = args.o.split('.')[-1]
    if outputfile_file_type == 'csv':
        logger.info('Reduced dimensional dataset has been saved in the {}.'.format(csvfile))

    elif outputfile_file_type == 'arff':
        df = pd.read_csv(csvfile, engine='python')
        filename, ext = os.path.splitext(args.i)

        if df['class'].dtype == np.float:
            df['class'] = df['class'].astype(int)
        temp = df['class']
        df = df.drop(columns=['class'], axis=1)
        df['class'] = temp
        DimensionReduction_filename = os.path.abspath('./Results') + os.sep + args.o
        pandas2arff.pandas2arff(df, filename=r'./Results/{}'.format(args.o), wekaname=filename, cleanstringdata=False,
                                cleannan=True)

        logger.info('Reduced dimensional dataset has been saved in the {}.'.format(DimensionReduction_filename))
        # clean_csv(csvfile)

    elif outputfile_file_type == 'libsvm':
        df = pd.read_csv(csvfile, engine='python')
        for x in df.columns:
            if x.lower() == 'class':
                label = x
                break

        y = df[label]
        X = df.drop(columns=label, axis=1)

        inputfile = args.i
        # filename ,ext = os.path.splitext(inputfile)
        DimensionReduction_filename = os.path.abspath('./Results') + os.sep + args.o
        dump_svmlight_file(X, y, DimensionReduction_filename, zero_based=True, multilabel=False)
        # clean_tmpfile.clean_csv(csvfile)
        logger.info('Reduced dimensional dataset has been saved in the {}.'.format(DimensionReduction_filename))
    else:
        logger.info('Reduced dimensional dataset has been saved in the {}.'.format(csvfile))
    ### 特征趋势图
    def lineplot_(in_file):
        plt.clf()
        if in_file.split(".")[-1]== "arff":
            df = arff2csv_notfile(in_file)
        elif in_file.split(".")[-1]== "libsvm":
            df = libsvm2csv_notfile(in_file)
        else:
            df = pd.read_csv(in_file).iloc[:,1:]
        x = [x for x in range(len(df.index.values))]
        new_list = range(math.floor(min(x)), math.ceil(max(x)) + 1)
        if args.t == "auc":
            plt.plot(x,df["auc"])
        elif args.t == "accuracy" or args.t =="acc":
            plt.plot(x, df["accuracy"])
        elif args.t == "precision":
            plt.plot(x, df["precision"])
        elif args.t == "recall":
            plt.plot(x, df["recall"])
        else:
            plt.plot(x, df["f1"])

        plt.title(f"The metrics change according to the feature ranking result ({args.t})",fontsize=18)
        plt.xlabel('features num', fontsize=16)
        plt.ylabel(f'score', fontsize=16)

        plt.savefig(f'./Results/{ os.path.splitext(os.path.basename(csvfile))[0]}.line.png', dpi=300)

    lineplot_(metrics_file)
    #####小提琴图
    def violin(in_file,topn = 15):
        plt.clf()
        plt.subplot(1,1,1)
        scaler = StandardScaler()
        if in_file.split(".")[-1]== "arff":
            df = arff2csv_notfile(in_file)
        elif in_file.split(".")[-1]== "libsvm":
            df = libsvm2csv_notfile(in_file)
        else:
            df = pd.read_csv(in_file)
        if (len(df.columns))-1 <= topn:
            topn = len(df.columns)-1
        df = df.iloc[:,:topn+1]
        sns.set_theme(color_codes=True)

        # if length<=topn:
        #     length=topn

        label = df.pop(df.columns[0])
        label_list = []

        length = len(df.columns)
        for x in range(length):
            label_list.append(label)

        df.columns.values[:] = ["top" + str(x) for x in range(1, len(df.columns)+1)]

        df[df.columns] = scaler.fit_transform(df[df.columns].to_numpy())
        df = df.melt(var_name='features', value_name='vals')

        df["label"] = pd.concat(label_list, ignore_index=True)
        snsfig = sns.violinplot(x="features", y="vals", data=df, hue=df.columns[-1], palette="muted")
        plt.title(f"top {args.f} features")
        figure = snsfig.get_figure()
        figure.savefig(f'./Results/{ os.path.splitext(os.path.basename(csvfile))[0]}.violin.png', dpi=300)

    violin(csvfile,args.f)

    ####热图
    def heatmap_(in_file, topn = 15):
        plt.clf()
        plt.figure(figsize=(6,5))


        scaler = StandardScaler()
        if in_file.split(".")[-1]== "arff":
            df = arff2csv_notfile(in_file)
        elif in_file.split(".")[-1]== "libsvm":
            df = libsvm2csv_notfile(in_file)
        else:
            df = pd.read_csv(in_file)
        if topn>= len(df.columns)-1:
            topn = len(df.columns)-1
        df.columns.values[1:]= ["top"+ str(x)for x in range(1,len(df.columns))]
        df = df.iloc[:,:topn+1]  # + label
        sns.set_theme(color_codes=True)
        df[df.columns[1:]] = scaler.fit_transform(df[df.columns[1:]].to_numpy())
        # method = ['pearson', 'kendall', 'spearman']
        df_corr = df.corr()

        sns_fig = sns.heatmap(df_corr, cmap="mako")
        figure = sns_fig.get_figure()
        plt.title(f"top {args.f} features")
        figure.savefig(
            f'./Results/{os.path.splitext(os.path.basename(csvfile))[0]}.heatmap.png',
            dpi=300)

        plt.show()

    heatmap_(csvfile,args.f)

    ###stem
    def feature_importance_stem_(topn = 15):
        plt.clf()
        xlist = []
        ylist = []
        if topn>= len(features_sorted):
            topn = len(features_sorted)
        xlist = ["top" + str(x) for x in range(1, topn+1)]

        for x, y in features_sorted[:topn]:

            ylist.append(y)

        plt.xlabel(f'feature score({args.r})', fontsize=18)
        plt.ylabel('features', fontsize=18)
        plt.title(f"top {args.f} features",fontsize=18)
        plt.stem(xlist[::-1], ylist[::-1], orientation='horizontal', linefmt='--', markerfmt='D')
        plt.savefig( f'./Results/{os.path.splitext(os.path.basename(csvfile))[0]}.stem.png',
            dpi=300)

    feature_importance_stem_()

    logger.info("---mrmd 3.0 end---")

